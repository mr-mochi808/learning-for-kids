# ğŸ—ï¸ Web Crawler Architecture

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Interfaces                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Web UI      â”‚  CLI Tool    â”‚  JavaScript   â”‚  Node.js     â”‚
â”‚ (index.html) â”‚  (cli.js)    â”‚  API          â”‚  Required    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Crawler Engine                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  WebCrawler        â”‚  AdvancedCrawler (extends)            â”‚
â”‚  (crawler.js)      â”‚  (advanced-crawler.js)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ URL Management   â”‚ â€¢ URL Filtering                       â”‚
â”‚ â€¢ robots.txt Check â”‚ â€¢ SEO Analysis                        â”‚
â”‚ â€¢ Link Discovery   â”‚ â€¢ Export Formats                      â”‚
â”‚ â€¢ Content Index    â”‚ â€¢ Performance Metrics                 â”‚
â”‚ â€¢ Search          â”‚ â€¢ Retry Logic                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HTTP & HTML Processing                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  axios             â”‚  cheerio         â”‚  robots-parser    â”‚
â”‚  (HTTP requests)   â”‚  (HTML parsing)  â”‚  (robots.txt)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Internet                                 â”‚
â”‚              (Target Websites)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Breakdown

### 1. WebCrawler Class (Core)

**File**: `crawler.js`

**Key Methods**:

```
WebCrawler
â”œâ”€â”€ crawl()                 # Main crawling loop
â”œâ”€â”€ fetchPage()            # Download HTML
â”œâ”€â”€ extractContent()       # Parse & index page
â”œâ”€â”€ extractLinks()         # Find all links
â”œâ”€â”€ search()              # Search indexed content
â”œâ”€â”€ isSameDomain()        # Check domain matching
â”œâ”€â”€ isUrlAllowed()        # Check robots.txt
â”œâ”€â”€ normalizeUrl()        # Standardize URLs
â”œâ”€â”€ getRobotRules()       # Parse robots.txt
â”œâ”€â”€ saveIndex()           # Export to JSON
â””â”€â”€ printStats()          # Show statistics
```

**Data Flow**:

```
startUrls
   â†“
[queue]
   â†“
fetchPage() â†’ extractContent() â†’ [index]
   â†“
extractLinks() â†’ filter & normalize
   â†“
add to queue
   â†“
repeat until done
```

### 2. AdvancedCrawler Class (Extensions)

**File**: `advanced-crawler.js`

**Additional Methods**:

```
AdvancedCrawler (extends WebCrawler)
â”œâ”€â”€ addFilter()            # Add URL patterns
â”œâ”€â”€ matchesFilters()       # Check URL against filters
â”œâ”€â”€ analyzeSEO()          # SEO metrics per page
â”œâ”€â”€ getSitemap()          # Hierarchical structure
â”œâ”€â”€ getMetrics()          # Performance stats
â”œâ”€â”€ exportAs()            # Export to JSON/CSV/XML
â”œâ”€â”€ exportCSV()           # CSV formatter
â”œâ”€â”€ exportXML()           # XML formatter
â””â”€â”€ fetchPageWithRetry()  # Retry failed requests
```

### 3. CLI Interface

**File**: `cli.js`

**Commands**:

```
cli.js
â”œâ”€â”€ crawl [url] [pages] [depth]     # Start crawling
â”œâ”€â”€ search <query> [indexFile]      # Search indexed data
â”œâ”€â”€ export <format> [filename]      # Export results
â”œâ”€â”€ analyze [indexFile]             # SEO analysis
â””â”€â”€ help                            # Show help
```

**Interactive Mode**: Prompts user for settings

### 4. Web Dashboard

**File**: `index.html`

**Features**:

```
Dashboard
â”œâ”€â”€ Configuration Panel
â”‚   â”œâ”€â”€ Start URL input
â”‚   â”œâ”€â”€ Max Pages slider
â”‚   â”œâ”€â”€ Max Depth slider
â”‚   â””â”€â”€ Timeout setting
â”œâ”€â”€ Statistics Panel
â”‚   â”œâ”€â”€ Pages Crawled
â”‚   â”œâ”€â”€ Pages Indexed
â”‚   â”œâ”€â”€ Error Count
â”‚   â””â”€â”€ Avg Time/Page
â”œâ”€â”€ Search Box
â”‚   â”œâ”€â”€ Query input
â”‚   â””â”€â”€ Search button
â””â”€â”€ Results Display
    â”œâ”€â”€ Results list
    â”œâ”€â”€ Title display
    â”œâ”€â”€ URL display
    â””â”€â”€ Snippet preview
```

## Data Structures

### Page Object

```javascript
{
  url: string,                    // Full URL
  title: string,                  // Page title
  description: string,            // Meta description
  keywords: string,               // Meta keywords
  headings: Array<{               // All headings
    level: 'h1'|'h2'|'h3',
    text: string
  }>,
  paragraphs: string[],           // Text content
  images: Array<{                 // Images found
    src: string,
    alt: string
  }>,
  links: Array<{                  // Links found
    text: string,
    href: string
  }>,
  language: string,               // HTML lang attribute
  crawledAt: ISO8601String        // Crawl timestamp
}
```

### Search Result Object

```javascript
{
  url: string,                    // Page URL
  title: string,                  // Page title
  description: string,            // Meta description
  relevance: number               // Relevance score
}
```

### Crawl Options

```javascript
{
  startUrls: string[],            // Starting URLs
  maxPages: number,               // Page limit
  maxDepth: number,               // Crawl depth
  timeout: number,                // Request timeout (ms)
  userAgent: string               // Custom User-Agent
}
```

## Crawling Algorithm

```
ALGORITHM: WebCrawl(startUrls, maxPages, maxDepth)
  
  INITIALIZE:
    queue â† startUrls
    visited â† empty set
    index â† empty array
    
  WHILE queue not empty AND visited.size < maxPages:
    
    url, depth â† queue.pop()
    
    IF url in visited:
      continue
    
    IF checkRobotsTxt(url) == DISALLOWED:
      log("Blocked by robots.txt")
      continue
    
    visited.add(url)
    
    TRY:
      html â† fetchPage(url)
      content â† extractContent(html, url)
      index.append(content)
      
      IF depth < maxDepth:
        links â† extractLinks(html, url)
        FOR EACH link in links:
          normalized â† normalizeUrl(link)
          IF sameDomain(url, normalized):
            queue.append((normalized, depth + 1))
    
    CATCH error:
      logError(url, error)
    
    WAIT (crawlDelay)  // Polite crawling
  
  RETURN index
```

## robots.txt Handling

```
â”Œâ”€ Request to crawl URL
â”‚
â”œâ”€ Extract domain
â”‚
â”œâ”€ Check robots cache
â”‚  â”œâ”€ If cached: Use cached rules
â”‚  â””â”€ If not: Fetch /robots.txt
â”‚
â”œâ”€ Parse robots.txt
â”‚  â”œâ”€ Find User-Agent: WebCrawler/1.0
â”‚  â”œâ”€ Check Disallow paths
â”‚  â””â”€ Extract Crawl-Delay
â”‚
â”œâ”€ Check if URL matches Disallow
â”‚  â”œâ”€ If allowed: Proceed with crawl
â”‚  â””â”€ If disallowed: Skip URL
â”‚
â””â”€ Apply Crawl-Delay if specified
```

## Search Algorithm

```
ALGORITHM: Search(query, index)
  
  results â† empty array
  
  FOR EACH page in index:
    score â† 0
    
    // Title match (weight: 10)
    IF query in page.title:
      score += 10
    
    // Description match (weight: 5)
    IF query in page.description:
      score += 5
    
    // Heading match (weight: 3)
    FOR EACH heading in page.headings:
      IF query in heading.text:
        score += 3
    
    // Paragraph match (weight: 1)
    FOR EACH paragraph in page.paragraphs:
      IF query in paragraph:
        score += 1
    
    IF score > 0:
      results.append({
        url: page.url,
        title: page.title,
        description: page.description,
        relevance: score
      })
  
  SORT results BY relevance DESC
  RETURN results
```

## Error Handling

```
Try to Fetch URL
â”‚
â”œâ”€ Network Error
â”‚  â”œâ”€ Retry (up to 3x)
â”‚  â””â”€ Log error
â”‚
â”œâ”€ Timeout
â”‚  â”œâ”€ Increase timeout on retry
â”‚  â””â”€ Log timeout
â”‚
â”œâ”€ 404 Not Found
â”‚  â”œâ”€ Skip page
â”‚  â””â”€ Log 404
â”‚
â”œâ”€ 403 Forbidden
â”‚  â”œâ”€ Check robots.txt
â”‚  â””â”€ Skip page
â”‚
â”œâ”€ Invalid HTML
â”‚  â”œâ”€ Parse partial content
â”‚  â””â”€ Log warning
â”‚
â””â”€ Other Errors
   â”œâ”€ Log error details
   â””â”€ Continue crawling
```

## Memory Management

### Per-Page Memory
- URL: ~100 bytes
- Title: ~100 bytes
- Description: ~150 bytes
- Content (avg): ~5-10 KB
- **Total: ~5-15 KB per page**

### Typical Usage
- 50 pages: ~250-750 KB
- 100 pages: ~500 KB - 1.5 MB
- 500 pages: ~2.5-7.5 MB

### Optimization Tips
1. Process results incrementally
2. Clear visited set periodically
3. Use streams for large exports
4. Implement caching layer

## Performance Considerations

### Request Timeline
```
URL Added
   â†“ (waiting in queue)
Fetch Started (axios)
   â”œâ”€ DNS lookup: 50-100ms
   â”œâ”€ TCP connect: 50-200ms
   â”œâ”€ TLS handshake: 100-500ms
   â”œâ”€ HTTP request: 10-50ms
   â”œâ”€ Server processing: 100-1000ms
   â”œâ”€ Response download: 10-100ms (depends on size)
   â””â”€ Total: 200-2000ms
   â†“
Parse HTML (cheerio)
   â”œâ”€ Load HTML: 10-50ms
   â”œâ”€ Extract content: 20-100ms
   â””â”€ Total: 30-150ms
   â†“
Add to Index
   â†“ (500ms crawl delay)
Next URL
```

## Concurrency

**Current**: Single-threaded, sequential processing
- Safer, more stable
- Respects crawl delays
- Easier to debug

**Future Enhancement**: Could implement:
- Worker threads for parallel crawling
- Connection pooling
- Rate limiting per domain

## Security Considerations

```
Input Validation:
â”œâ”€ URL validation (URL parsing)
â”œâ”€ HTML sanitization (cheerio safe)
â”œâ”€ robots.txt parsing (robots-parser)
â””â”€ User agent sanitization

Resource Limits:
â”œâ”€ Request timeout
â”œâ”€ Page size limits
â”œâ”€ Max depth restriction
â””â”€ Max pages cap

Network Security:
â”œâ”€ HTTPS support (automatic)
â”œâ”€ SSL verification
â”œâ”€ Redirect limits (5 max)
â””â”€ User-Agent identification
```

## File Organization

```
proxyy/
â”œâ”€â”€ Core Engine
â”‚   â”œâ”€â”€ crawler.js          (280+ lines)
â”‚   â””â”€â”€ advanced-crawler.js (250+ lines)
â”œâ”€â”€ User Interfaces
â”‚   â”œâ”€â”€ cli.js              (150+ lines)
â”‚   â”œâ”€â”€ index.html          (400+ lines)
â”‚   â””â”€â”€ examples.js         (80+ lines)
â”œâ”€â”€ Documentation
â”‚   â”œâ”€â”€ README.md           (400+ lines)
â”‚   â”œâ”€â”€ QUICKSTART.md       (300+ lines)
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md (200+ lines)
â”‚   â””â”€â”€ ARCHITECTURE.md     (this file)
â”œâ”€â”€ Testing & Config
â”‚   â”œâ”€â”€ test.js             (250+ lines)
â”‚   â”œâ”€â”€ config.json         (50 lines)
â”‚   â””â”€â”€ package.json        (20 lines)
â””â”€â”€ Setup
    â””â”€â”€ setup.bat           (30 lines)

Total: 3000+ lines of code and documentation
```

## Workflow Diagram

```
â”Œâ”€ User Starts Crawler
â”‚
â”œâ”€ Load Config
â”‚
â”œâ”€ Validate Start URLs
â”‚
â”œâ”€ Initialize Queue
â”‚
â””â”€ CRAWLING LOOP
   â”‚
   â”œâ”€ While queue not empty
   â”‚  â”‚
   â”‚  â”œâ”€ Pop URL from queue
   â”‚  â”‚
   â”‚  â”œâ”€ Check robots.txt
   â”‚  â”‚
   â”‚  â”œâ”€ Fetch page
   â”‚  â”‚
   â”‚  â”œâ”€ Parse HTML
   â”‚  â”‚
   â”‚  â”œâ”€ Extract content
   â”‚  â”‚
   â”‚  â”œâ”€ Add to index
   â”‚  â”‚
   â”‚  â”œâ”€ Extract links
   â”‚  â”‚
   â”‚  â”œâ”€ Filter & normalize
   â”‚  â”‚
   â”‚  â”œâ”€ Add to queue
   â”‚  â”‚
   â”‚  â””â”€ Wait (crawl delay)
   â”‚
   â””â”€ Generate Results
      â”‚
      â”œâ”€ Save index.json
      â”‚
      â”œâ”€ Show statistics
      â”‚
      â”œâ”€ Enable search
      â”‚
      â””â”€ Allow export
```

---

**Architecture Version**: 1.0
**Last Updated**: December 5, 2025
